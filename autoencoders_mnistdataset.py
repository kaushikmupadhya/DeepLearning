# -*- coding: utf-8 -*-
"""AutoEncoders_MNISTDataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pvpJ55mdf-4pIUrJZMcJzqrkPqxC6qS9

###**Auto Encoders By Kaushik Manjunatha**
1. Unsupervised Learning.
2. To compress the data.
3. Used in compression , denoising and anomaly detection
"""

#Import Statements
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers, losses
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Model

#Loading the MNIST Dataset (loading to Xtrain and Xtest, because it is unsupervised learning)
(x_train, _), (x_test, _) = mnist.load_data()

#Training first 10000 images and rest in validation set
x_train, x_val = x_train[:-10000], x_train[-10000:]

x_train = x_train.astype('float32')/255
x_test = x_test.astype('float32')/255
x_val = x_val.astype('float32')/255

print(x_train.shape)
print(x_test.shape)
print(x_val.shape)

#Visualisation of Images using matplotlib
n = 10
plt.figure(figsize= (20, 4))
for i in range(n):
  #displaying original images
  ax = plt.subplot(2, n, i+1)
  plt.imshow(x_test[i])
  plt.title("Original")
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

#Defining the Autoencoder class
latent_dimension = 64

class Autoencoder(Model):
  def __init__(self, latent_dimension): #defining the constructer
    super(Autoencoder, self).__init__()
    self.latent_dimension = latent_dimension
    #defining encoder
    self.encoder = tf.keras.Sequential([
                                        layers.Flatten(),
                                        layers.Dense(latent_dimension, activation = 'relu'),
    ])
    #defining deencoder
    self.decoder = tf.keras.Sequential([
                                        layers.Dense(784, activation = 'sigmoid'), #we want output from 0 to 1
                                        layers.Reshape((28, 28))
    ])

  def call(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    return decoded

autoencoder = Autoencoder(latent_dimension)

#Training the Model
autoencoder.compile(optimizer= 'adam', loss = losses.MeanSquaredError())

autoencoder.fit(x_train, x_train,
                epochs = 10,
                shuffle = True,
                validation_data = (x_val, x_val))

print(autoencoder.encoder.summary())

print(autoencoder.decoder.summary())

encoded_imgs = autoencoder.encoder(x_test).numpy()
decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()

n = 10
plt.figure(figsize= (20, 4))
for i in range(n):
  #displaying original images
  ax = plt.subplot(2, n, i+1)
  plt.imshow(x_test[i])
  plt.title("Original")
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

  #displaying reconstructed images
  ay = plt.subplot(2, n, i+1+n)
  plt.imshow(decoded_imgs[i])
  plt.title("Reconstructed")
  plt.gray()
  ay.get_xaxis().set_visible(False)
  ay.get_yaxis().set_visible(False)
plt.show()

